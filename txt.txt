Je vais décrire une pipeline complète qui analyse des données crypto pour créer des insights intéressants.
Sources de données :

CoinGecko API : données historiques de prix, volumes, market cap
Etherscan API : transactions Ethereum, gas fees, smart contracts
DefiLlama API : TVL (Total Value Locked) par protocole
Ces APIs sont gratuites avec des limites raisonnables.

Architecture :

Ingestion :

Script Python avec Apache Airflow sur une VM GCP
Extraction quotidienne via les APIs
Stockage initial dans un Cloud Storage bucket (données brutes)


Processing avec PySpark :

Dataproc comme service managé pour Spark
Connection à GCP via les connecteurs natifs spark.hadoop.fs.*
Transformation des données :

Nettoyage/standardisation des timestamps
Calcul de métriques : volatilité, corrélations entre coins
Agrégations par période (daily, weekly, monthly)
Jointures entre sources pour insights croisés (ex: impact des gas fees sur le volume)




Stockage :

Tables dans BigQuery pour les données transformées
Partitionnement par date pour optimiser les requêtes
Tables différentes pour données raw vs agrégées


Visualisation :

Connection de PowerBI à BigQuery via connecteur natif
Dashboard avec :

Vue globale du marché (volumes, tendances)
Métriques par protocole
Alertes sur variations importantes
Prédictions simples (moving averages)





Points techniques importants :

Authentification GCP via service account keys
PySpark configs pour optimiser la mémoire/partitions
Monitoring des jobs Airflow
Gestion des coûts (scaling Dataproc)